{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install opustools\n",
        "!pip install lingtrain_aligner\n",
        "!pip install razdel\n",
        "!pip install dataparser\n",
        "!pip install dateparser\n",
        "!pip install sentence_transformers\n",
        "!pip3 install nltk\n",
        "!pip install xmltodict\n",
        "!pip install pypinyin\n",
        "!pip install pykakasi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8T8yOjZuRV",
        "outputId": "e8405feb-2508-4a15-952f-bb98506630da"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opustools in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lingtrain_aligner in /usr/local/lib/python3.7/dist-packages (0.7.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: razdel in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dataparser in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2022.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,<2022.3.15 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2022.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.20.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "qrRG5lD8Y-Po"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, SentencesDataset, LoggingHandler, losses\n",
        "from sentence_transformers.readers import InputExample\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "import math, re\n",
        "from transformers import MBartForConditionalGeneration as Model\n",
        "from transformers import MBart50TokenizerFast as Tokenizer\n",
        "import datasets\n",
        "import torch\n",
        "from random import choice\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from xml.dom.minidom import parse\n",
        "import opustools\n",
        "from lingtrain_aligner import splitter, aligner, resolver, metrics\n",
        "import nltk\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1984, Orwell"
      ],
      "metadata": {
        "id": "GvpbP7BQh5Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip MTE1984-ana.zip"
      ],
      "metadata": {
        "id": "8wHijsebh8xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_alignment(alignment, t1):\n",
        "    tree = parse(alignment)\n",
        "    first_lang, second_lang = [], []\n",
        "\n",
        "    links = tree.getElementsByTagName(\"link\")\n",
        "    for link in links:\n",
        "        targets = dict(link.attributes.items())['targets'].split()\n",
        "        s1, s2 = [], []\n",
        "        for tgt in targets:\n",
        "            ts = tgt.split(\"#\")\n",
        "            # if ts[0] == 'oana-en.xml':\n",
        "            if ts[0] == t1:\n",
        "                s1.append(ts[1])\n",
        "            else:\n",
        "                s2.append(ts[1])\n",
        "        first_lang.append(s1)\n",
        "        second_lang.append(s2)\n",
        "\n",
        "    return first_lang, second_lang\n",
        "\n",
        "def parse_lang_xml(filename, align_list):\n",
        "    line_num, cur_str, strr, strings = 0, 0, \"\", []\n",
        "    tree = parse(filename)\n",
        "    paragrs = tree.getElementsByTagName(\"p\")\n",
        "    for p in paragrs:\n",
        "        for st in p.childNodes:\n",
        "            if st.nodeType is not node.TEXT_NODE:\n",
        "                s = \"\"\n",
        "                # print(st)\n",
        "                if st.nodeType is not node.TEXT_NODE:\n",
        "                    # print(dict(st.attributes.items()))\n",
        "                    str_id = dict(st.attributes.items())['xml:id']\n",
        "                    for w in st.childNodes:\n",
        "                        if w.nodeType is not node.TEXT_NODE:\n",
        "                            for n in w.childNodes:\n",
        "                                s += n.data\n",
        "                                s += \" \"\n",
        "                    # print(s)\n",
        "                    if line_num < len(align_list) and str_id not in align_list[line_num] and cur_str > 0:\n",
        "                        strings.append(strr)\n",
        "                        # print(strr)\n",
        "                        strr = \"\"\n",
        "                        cur_str = 0\n",
        "                        if line_num < len(align_list):\n",
        "                            line_num += 1\n",
        "                    if line_num < len(align_list) and str_id not in align_list[line_num] and cur_str == 0:\n",
        "                        s = \"\"\n",
        "                    if line_num < len(align_list) and len(align_list[line_num]) == 1 and align_list[line_num][0] == str_id:\n",
        "                        if line_num < len(align_list):\n",
        "                            line_num += 1\n",
        "                        # print(s)\n",
        "                        strings.append(s)\n",
        "                    elif line_num < len(align_list) and len(align_list[line_num]) > 1 and str_id in align_list[line_num]:\n",
        "                        strr += s\n",
        "                        cur_str += 1\n",
        "    return strings\n",
        "\n",
        "def write_res(strings, filename):\n",
        "    of = open(filename, \"w\")\n",
        "    for s in strings:\n",
        "        of.write(s)\n",
        "        of.write(\"\\n\")\n",
        "    of.close()\n",
        "\n",
        "def parse_xml(alignment, lang1, lang2, t1, t2, xml1, xml2):\n",
        "    first_lang, second_lang = parse_alignment(alignment, t1)\n",
        "    print(len(first_lang))\n",
        "    print(len(second_lang))\n",
        "\n",
        "    # parse xmls according to the alignment\n",
        "    strings = parse_lang_xml(xml1, first_lang)\n",
        "    write_res(strings, lang1)\n",
        "\n",
        "    strings = parse_lang_xml(xml2, second_lang)\n",
        "    write_res(strings, lang2)"
      ],
      "metadata": {
        "id": "VtYuqxpziIKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse_xml('/content/MTE1984-ana/oalg-enro.xml', '/content/en1.txt', '/content/ro1.txt', 'oana-en.xml', 'oana-ro.xml', \"/content/MTE1984-ana/oana-en.xml\", \"/content/MTE1984-ana/oana-ro.xml\")\n",
        "# parse_xml('/content/MTE1984-ana/oalg-bgen.xml', '/content/en.txt', '/content/bg.txt', 'oana-en.xml', 'oana-bg.xml', \"/content/MTE1984-ana/oana-bg.xml\", \"/content/MTE1984-ana/oana-en.xml\")\n",
        "# parse_xml('/content/MTE1984-ana/oalg-enhu.xml', '/content/en.txt', '/content/hu.txt', 'oana-en.xml', 'oana-hu.xml', \"/content/MTE1984-ana/oana-en.xml\", \"/content/MTE1984-ana/oana-hu.xml\")"
      ],
      "metadata": {
        "id": "kOQLC4dCiPER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jane Air"
      ],
      "metadata": {
        "id": "yg1WPmwliYlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extraction**"
      ],
      "metadata": {
        "id": "3nJvBSuijq1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !opus_read --directory Books --source de --target es -m 5500 --write tmp.txt"
      ],
      "metadata": {
        "id": "NDL6cKJtiaNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book = open('/content/tmp.txt', \"r\")\n",
        "cleaned = open('/content/tmp1.txt', \"w\")\n",
        "lines = book.readlines()\n",
        "\n",
        "sources, targets = [], []\n",
        "\n",
        "s_s = \"\"\n",
        "s_t = \"\"\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip('\\n')\n",
        "    if '(src)' in line:\n",
        "        s_s += line.split('>')[1]\n",
        "        s_s += \" \"\n",
        "    elif '(trg)' in line:\n",
        "        s_t += line.split('>')[1]\n",
        "        s_t += \" \"\n",
        "    elif '==' in line:\n",
        "        sources.append(s_s)\n",
        "        targets.append(s_t)\n",
        "        s_s = \"\"\n",
        "        s_t = \"\"\n",
        "\n",
        "write_res(sources, '/content/de-es.de')\n",
        "write_res(targets, '/content/de-es.es')"
      ],
      "metadata": {
        "id": "KA9JpOb3j49B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alignment**"
      ],
      "metadata": {
        "id": "wTeJmAbOirTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text1, text2, text3, text4 are vars with text"
      ],
      "metadata": {
        "id": "OmTZXTpkjPFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('LaBSE')\n",
        "\n",
        "lang_from = \"de\"\n",
        "lang_to = \"es\"\n",
        "db_path = \"alignment.db\"\n",
        "# lang_from = \"de\"\n",
        "# lang_to = \"en\"\n",
        "\n",
        "# splitted_from = splitter.split_by_sentences(text1.split('\\n'), lang_from)\n",
        "# splitted_to = splitter.split_by_sentences(text2.split('\\n'), lang_to)\n",
        "aligner.fill_db(db_path, lang_from, lang_to, splitted_from, splitted_to)\n",
        "\n",
        "steps = 3\n",
        "\n",
        "for i in range(steps):\n",
        "    conflicts, rest = resolver.get_all_conflicts(db_path,\n",
        "                        min_chain_length=2+i,\n",
        "                        max_conflicts_len=6*(i+1),\n",
        "                        batch_id=-1)\n",
        "\n",
        "    resolver.resolve_all_conflicts(db_path, conflicts, \"sentence_transformer_multilingual_labse\", show_logs=False)\n",
        "\n",
        "    if len(rest) == 0:\n",
        "        break\n",
        "\n",
        "output_path=\"/content\"\n",
        "\n",
        "saver.save_plain_text(db_path, os.path.join(output_path, f\"corpora_{lang_from}.txt\"), direction=\"from\", batch_ids=[])\n",
        "saver.save_plain_text(db_path, os.path.join(output_path, f\"corpora_{lang_to}.txt\"), direction=\"to\", batch_ids=[])\n"
      ],
      "metadata": {
        "id": "sk2d4C8Biu0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligner.align_db(db_path,\n",
        "                model_name=\"sentence_transformer_multilingual_labse\",\n",
        "                batch_size=200,\n",
        "                window=50,\n",
        "                batch_ids=[],\n",
        "                save_pic=False,\n",
        "                embed_batch_size=5,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=True,\n",
        "                shift=0)"
      ],
      "metadata": {
        "id": "SDE3kTO6jMRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translation**"
      ],
      "metadata": {
        "id": "WWtDPRT3kA2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, refs, lang, md):\n",
        "    tokenizer.src_lang = lang\n",
        "    result = []\n",
        "    res_list = []\n",
        "    refs_list = []\n",
        "    \n",
        "    for i, t in enumerate(text):\n",
        "        if i % 10 == 0:\n",
        "            print(i)\n",
        "        encoded = tokenizer(t, return_tensors=\"pt\")\n",
        "        if md == 0:\n",
        "            generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
        "        else:\n",
        "            generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
        "        res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        \n",
        "        res_list.append(re.sub(\"[^\\w]\", \" \",  res[0]).split())\n",
        "        refs_list.append([re.sub(\"[^\\w]\", \" \",  refs[i]).split()])\n",
        "        result.append(res[0])\n",
        "    return res_list, refs_list, result"
      ],
      "metadata": {
        "id": "6ss2pevcjXGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_bleu(lang1, lang2, mod, rs, hs):\n",
        "    refs = open(rs, \"r\").readlines() # human translation\n",
        "    hyps = open(hs, \"r\").readlines() # machine translation\n",
        "\n",
        "    refs_list, hyps_list = [], []\n",
        "\n",
        "    s = \"\"\n",
        "    s += lang1 \n",
        "    s += \" - \" \n",
        "    s += lang2 \n",
        "    s += \" - \" \n",
        "    s += mod\n",
        "    print(s)\n",
        "\n",
        "    for i, ref in enumerate(refs):\n",
        "        # print([re.sub(\"[^\\w]\", \" \",  refs[i]).split()])\n",
        "        refs_list.append([re.sub(\"[^\\w]\", \" \",  refs[i]).split()])\n",
        "        hyps_list.append(re.sub(\"[^\\w]\", \" \",  hyps[i]).split())\n",
        "        smoothie = SmoothingFunction().method4\n",
        "    score = nltk.translate.bleu_score.corpus_bleu(refs_list, hyps_list)\n",
        "    print(score)"
      ],
      "metadata": {
        "id": "0QyWZ8oWHR5U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_samples(sample_orig, sample_eup_tr):\n",
        "    orig_sample = open(sample_orig, \"r\")\n",
        "    tr_eu_sample = open(sample_eup_tr, \"r\")\n",
        "    \n",
        "    sample = orig_sample.readlines()\n",
        "    sample_refs = tr_eu_sample.readlines()\n",
        "    \n",
        "    orig_sample.close()\n",
        "    tr_eu_sample.close()\n",
        "        \n",
        "    return sample, sample_refs"
      ],
      "metadata": {
        "id": "He3zyFVEkbEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_and_write(lang, trans, md, sample, sample_refs):\n",
        "    rsl, rfl, rs = translate(sample, sample_refs, lang, md)\n",
        "    trans_sample = open(trans, \"w\")\n",
        "    for r in rs:\n",
        "        trans_sample.write(r)\n",
        "        trans_sample.write(\"\\n\")\n",
        "    trans_sample.close()"
      ],
      "metadata": {
        "id": "CEok8bJxmn1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")    \n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "sample1, sample_refs1 = prepare_samples(\"de-es_ja.es\", \"de-es_ja.de\")\n",
        "sample2, sample_refs2 = prepare_samples(\"de-en_ja.en\", \"de-en_ja.de\")\n",
        "\n",
        "print(\"es-de\")\n",
        "tr_and_write(\"es\", \"es_translated_m2m.txt\", 0, sample1, sample_refs1)\n",
        "\n",
        "print(\"en-de\")\n",
        "tr_and_write(\"en\", \"en_translated_m2m.txt\", 0, sample2, sample_refs2)\n",
        "\n",
        "model = Model.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")    \n",
        "tokenizer = Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "print(\"es-de\")\n",
        "result = count_bleu(\"es_XX\", \"es_translated_bart.txt\", 1, sample1, sample_refs1)\n",
        "print(result['bleu'])\n",
        "\n",
        "print(\"en-de\")\n",
        "result = count_bleu(\"en_XX\", \"es_translated_bart.txt\", 1, sample2, sample_refs2)\n",
        "print(result['bleu'])"
      ],
      "metadata": {
        "id": "_nR9B1hClU3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_bleu(\"en\", \"de\", \"m2m\", \"/content/de-en_ja.de\", \"/content/en_translated_m2m.txt\")\n",
        "count_bleu(\"es\", \"de\", \"m2m\", \"/content/de-es_ja.de\", \"/content/es_translated_m2m.txt\")\n",
        "count_bleu(\"en\", \"de\", \"mBart\", \"/content/de-en_ja.de\", \"/content/en_translated_bart.txt\")\n",
        "count_bleu(\"es\", \"de\", \"mBart\", \"/content/de-es_ja.de\", \"/content/es_translated_bart.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU5FI8WDZiI0",
        "outputId": "cbbf990e-e62e-4704-ed2f-0db7546b9573"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en - de - m2m\n",
            "0.07118307490841326\n",
            "es - de - m2m\n",
            "0.014667333605911407\n",
            "en - de - mBart\n",
            "0.08086458114310201\n",
            "es - de - mBart\n",
            "0.008181394008549434\n"
          ]
        }
      ]
    }
  ]
}